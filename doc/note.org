#+LATEX_HEADER: \usepackage[slantfont, boldfont]{xeCJK}
#+LATEX_HEADER: \setCJKmainfont{WenQuanYi Micro Hei}
#+LATEX_HEADER: \setCJKsansfont{WenQuanYi Micro Hei}
#+LATEX_HEADER: \setCJKmonofont{WenQuanYi Micro Hei Mono}
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \RequirePackage[left=2.5cm,right=2.5cm,bottom=2.5cm,top=2.5cm, headheight=1.5cm, footskip=1cm]{geometry}
#+LATEX_HEADER: \title{hello，org-mode!}

#+TITLE: Notes for Machine Learning
#+AUTHOR: Jack Wang
#+EMAIL: cnglen@gmail.com

* Introducition (1)
** Definition of Machine Learning (ML)
- TEP :: A computer program is said to learn from experience *E* with
     respect to some task *T* and some performance measure *P* if its
     performance on *T*, as measured by *P*, _improves with experience
     *E*_.
- Machine learning :: the field of study that gives computers the
     ability to learn without being explicitly programmed.

** Types of learning
   * Supervised learning :: Given {Features, Label} to estimate the label for new data.
     - regression (target variable is continuous)
     - classfication (target variable is discrete)
   * Un-supservised learning :: Given {Features} to do somthing, e.g, clustering. *No lables are given.*
     - clustering (image processing to 3D, _seprate sounds from two person(SVD)_ [fn:seprate_sound])
   * Reinforcement learning ::  inspired by behaviorist psychology,
        concerned with how software agents ought to take actions in an
        environment so as to maximize some notion of cumulative reward
     - robot

** Todo
   How to seprate sound from two person?

[fn:seprate_sound] Is it necessary to have two speakers? What if only one speaker and two person?


* Linear Regression (2,3,4,5)
** Regression

  #+CAPTION: Notation of Math symbols
  | Notation                                                                                                                                                                                  | Meaning                                                           |
  |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------|
  | $$\mathbf{x} = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in\mathbb{R}^{(n+1)\times 1} $$         | input variables, features                                         |
  | $y$                                                                                                                                                                                       | output/target variable                                            |
  | $\hat{y} = h(\mathbf{x}) = h_{\pmb{\theta}}(\mathbf{x})$                                                                                                                                  | hypothesis function of $\mathbf{x}$ with parameter $\pmb{\theta}$ |
  |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------|
  | $\mathbf{x}^{(i)} = \begin{bmatrix} x_0^{(i)} \\ x_1^{(i)} \\ \vdots  \\ x_n^{(i)} \end{bmatrix}$                                                                                         | $i^{th}$ observation of $\mathbf{x}$ where $1 \leq i \leq m$      |
  | $y^{(i)}$                                                                                                                                                                                 | $i^{th}$ observation of $y$ where $1 \leq i \leq m$               |
  | $\mathbf{y}= \begin{bmatrix}   y^{(1)} \\   y^{(2)} \\   \vdots \\   y^{(m)}   \end{bmatrix}\in \mathbb{R}^{m \times 1}$                                                                  | $m$ observations of $y$                                           |
  | $$\mathbf{X}= \begin{bmatrix}\big(\mathbf{x}^{(1)}\big)^{T} \\ \big(\mathbf{x}^{(2)}\big)^{T} \\ \vdots \\ \big(\mathbf{x}^{(m)}\big)^{T} \end{bmatrix} \in \mathbb{R}^{m \times (n+1)}$$ | $m$ observations of $\mathbf{x}$                                  |
  | $m$                                                                                                                                                                                       | # of traing examples                                              |
  | $n$                                                                                                                                                                                       | # of traing features (excluding addtional 1 vector)               |
  | $\pmb{\theta} = \begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix}$                                                                                                 | parameter of hypothesis funtion(model)                            |


  TEP defintion of regression:
    - E: Given $m$ observations: $(\mathbf{X}, \mathbf{y})$,
    - T: Predict $y$ using model $h(\mathbf{x}) = h_{\pmb{\theta}}(\mathbf{x})$,
    - P: which minimizes loss(error) fuction $J(\pmb{\theta})$

  After selecting the model (e.g, linear model), T becomes to find
$\arg \min_{\pmb{\theta}} J(\pmb{\theta})$ where:
\begin{align}
J(\pmb{\theta})
&= \sum_{i=1}^{m} \big(h_{\pmb{\theta}}(\mathbf{x}^{(i)}) - y^{(i)}\big)^2             && \textnormal{(SSE)} \\
&= \frac{1}{m}\sum_{i=1}^{m} \big(h_{\pmb{\theta}}(\mathbf{x}^{(i)}) - y^{(i)}\big)^2  && \textnormal{(MSE)}
\end{align}

\begin{equation}
\mathbf{X}
= \begin{bmatrix}
  1& x_{1}^{(1)}& x_{2}^{(1)}& \cdots& x_{n}^{(1)} \\
  1& x_{1}^{(2)}& x_{2}^{(2)}& \cdots& x_{n}^{(2)} \\
  \vdots& \vdots& \vdots&      \ddots& \vdots      \\
  1& x_{1}^{(m)}& x_{2}^{(m)}& \cdots& x_{n}^{(m)}
  \end{bmatrix}
= \begin{bmatrix}
  x_{0}^{(1)}& x_{1}^{(1)}& x_{2}^{(1)}& \cdots& x_{n}^{(1)} \\
  x_{0}^{(2)}& x_{1}^{(2)}& x_{2}^{(2)}& \cdots& x_{n}^{(2)} \\
  \vdots& \vdots& \vdots& \ddots& \vdots \\
  x_{0}^{(m)}& x_{1}^{(m)}& x_{2}^{(m)}& \cdots& x_{n}^{(m)}
  \end{bmatrix}
= \begin{bmatrix}
  \big(\mathbf{x}^{(1)}\big)^{T} \\
  \big(\mathbf{x}^{(2)}\big)^{T} \\
  \vdots \\
  \big(\mathbf{x}^{(m)}\big)^{T}
  \end{bmatrix}
\in \mathbb{R}^{m \times (n+1)}
\end{equation}

+ 解析解


+ Iteration method
  Gradicent decent method

+ Regression models:
  - linear model
    $h_{\pmb{\theta}}(\mathbf{x}) = \langle \pmb{\theta}, \mathbf{x} \rangle = \mathbf{x}^{T} \pmb{\theta}$
  - poly model

** Model and Cost function


** Parameter learning

Traing Set -> Learning Algorithem (Chosen) -> $h_{\theta}(x)=h(x)$


** Demo of Gradicent Decent

   All figures [[fig:J_n_iter]], [[fig:J_n_iter_ns]], [[fig:J_theta_3d]],
   [[fig:J_theta_3d_ns]], [[fig:J_theta_contour]] and [[fig:J_theta_contour_ns]]
   are generated by =../src/demo_gradient_descent.py=

   We can find:
   - No scaling, Gradicent Decent 在不同维度上的陡峭程度不一样（二维中
     轮廓图由圆变为椭圆，等高线越密集，越陡峭），越陡峭,下降降幅度越厉
     害，梯度下降法给予该陡峭维度的权重比较大，所以会沿着陡峭维度的主
     方向下降，之后是第二个陡峭的维度成为主方向，以此类推。与圆形轮廓
     图相比，误入“歧图”，多乖了个“弯儿"
   - Gradicent Decent can only find the *local minimum* point
   - The step size during update of $\pmf{\theta}$ will reduce
     automatically while approaching non-steep surface, even for the
     same *learning rate*

*** $J($ # of iterations $)$
    #+CAPTION: J(# of iterations)
    #+NAME: fig:J_n_iter
    #+ATTR_LATEX: :width 8cm
    [[../src/figure/J_n_iter.png]]

    #+CAPTION: J(# of iterations) without scaling
    #+NAME: fig:J_n_iter_ns
    #+ATTR_LATEX: :width 8cm
    [[../src/figure/J_n_iter_no_scaling.png]]
*** $J(\theta_0, \theta_1)$ in 3D
    #+CAPTION: $J(\theta_0, \theta_1)$ in 3D
    #+NAME: fig:J_theta_3d
    #+ATTR_LATEX: :width 8cm
    [[../src/figure/J_theta_3D.png]]

    #+CAPTION: $J(\theta_0, \theta_1)$ in 3D without scaling
    #+NAME: fig:J_theta_3d_ns
    #+ATTR_LATEX: :width 8cm
    [[../src/figure/J_theta_3D_no_scaling.png]]
*** $J(\theta_0, \theta_1)$ in contour

    #+CAPTION: $J(\theta_0, \theta_1)$ in contour
    #+NAME: fig:J_theta_contour
    #+ATTR_LATEX: :width 8cm
    [[../src/figure/J_theta_contour.png]]

    #+CAPTION: $J(\theta_0, \theta_1)$ in contour without scaling
    #+NAME: fig:J_theta_contour_ns
    #+ATTR_LATEX: :width 8cm
    [[../src/figure/J_theta_contour_no_scaling.png]]
